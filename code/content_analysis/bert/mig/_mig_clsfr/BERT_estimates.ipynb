{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kopie von Kopie von BERT_estimates.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/nicolaiberk/bild/blob/main/code/content_analysis/bert/mig/_mig_clsfr/BERT_estimates.ipynb",
      "authorship_tag": "ABX9TyN3x6qxqSISxqQadOQddm1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolaiberk/bild/blob/main/code/content_analysis/bert/mig/_mig_clsfr/BERT_estimates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UOsFRKzo-icJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2a9243-4749-438d-a846-b93603c29069"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzbQ7FSa-fQP",
        "outputId": "2571d784-4be4-444c-b49c-396b9695179a"
      },
      "source": [
        "!pip install transformers datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPPJA5sFAzrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a48047-8aaf-4fb0-9a9d-01c5a7a5c4c3"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from itertools import islice\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BO6jN_E9OSC"
      },
      "source": [
        "# get full set of news articles\n",
        "if not os.path.isfile('newspapers/_bild_articles.csv'):\n",
        "    os.system('wget -O articles.zip https://www.dropbox.com/sh/bbf0655w9931xbk/AADQNpkipxBENPk4Gp5j1UaDa?dl=0')\n",
        "    os.system('unzip articles.zip -d newspapers')\n",
        "    os.system('rm articles.zip')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "NPSXlCfcRXyH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeAcOBKxEXio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39827a9d-59e2-4ffd-b714-e79f3b6fd785"
      },
      "source": [
        "model_name = \"drive/MyDrive/Bild/mig_clsfr_BERT_torch\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.to(device)  # Move model to GPU\n",
        "model.eval()  # Set model to evaluation mode\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-german-cased', max_length = 512, truncation=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:86: UserWarning: \n",
            "Access to the secret `HF_TOKEN` has not been granted on this notebook.\n",
            "You will not be requested again.\n",
            "Please restart the session if you want to be prompted again.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "gUZzGX78TwG9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper = os.listdir('newspapers')[3]\n",
        "paper"
      ],
      "metadata": {
        "id": "pbSQ2YRpTgdx",
        "outputId": "fcf50acc-f413-4a8a-e9b5-631191684aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'_bild_articles_2019.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "CamByzo_mS4x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'newspapers/'+paper\n",
        "print(f'Processing file {paper}')\n",
        "dataset = load_dataset(\"csv\", data_files=filename, split=\"train\", streaming=True)"
      ],
      "metadata": {
        "id": "oO30rY9TTrO-",
        "outputId": "ab2be38a-7234-4107-f76e-1872ff19eb6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file _bild_articles_2019.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(batch):\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}  # Move inputs to GPU\n",
        "        outputs = model(**inputs)  # Model inference on GPU\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "UUYzpfZeoFPn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## remove empty texts\n",
        "dataset = dataset.filter(lambda example: example[\"text\"] is not None)"
      ],
      "metadata": {
        "id": "R0oP4bKSrWbG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "SkAzXJACkHvo",
        "outputId": "961c5f34-5fff-4a60-a691-4b34168258a0"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "for paper in os.listdir('newspapers'):\n",
        "  filename = 'newspapers/'+paper\n",
        "  print(f'Processing file {paper}')\n",
        "  with open(filename, 'r') as csvfile:\n",
        "    with open('drive/MyDrive/Bild/BERT_estimates.csv', mode='a') as fo:\n",
        "      reader = csv.reader(csvfile)\n",
        "      for row in reader:\n",
        "          # define relative position in row based on title\n",
        "          titlerow = np.argmax([r == 'title' for r in row])\n",
        "          linkrow  = np.argmax([r == 'url'   for r in row])\n",
        "          daterow  = np.argmax([r == 'date'  for r in row])\n",
        "          textrow  = np.argmax([r == 'text'  for r in row])\n",
        "          topicrow = np.argmax([r == 'topic' for r in row])\n",
        "          break\n",
        "\n",
        "      writer = csv.writer(fo)\n",
        "      writer.writerow(['title', 'link', 'date', 'topic', 'est', 'label'])\n",
        "\n",
        "      titlebatch = []\n",
        "      linkbatch = []\n",
        "      datebatch = []\n",
        "      textbatch = []\n",
        "      topicbatch = []\n",
        "\n",
        "      count = 0\n",
        "      batches_run = 0\n",
        "\n",
        "      for row in tqdm(reader):\n",
        "        if count >= batch_size:\n",
        "            estbatch = infer(textbatch)\n",
        "\n",
        "            for title, link, date, topic, est, label in zip(titlebatch, linkbatch, datebatch, topicbatch, estbatch, labelbatch):\n",
        "              writer.writerow([title, link, date, topic, est, label])\n",
        "\n",
        "            count = 0\n",
        "            titlebatch = []\n",
        "            linkbatch = []\n",
        "            datebatch = []\n",
        "            textbatch = []\n",
        "            topicbatch = []\n",
        "\n",
        "        titlebatch.append(row[titlerow])\n",
        "        linkbatch.append(row[linkrow])\n",
        "        datebatch.append(row[daterow])\n",
        "        textbatch.append(row[textrow])\n",
        "        topicbatch.append(row[topicrow])\n",
        "        count += 1\n",
        "\n",
        "      if len(titlebatch) > 0:\n",
        "        for title, link, date, topic, est, label in zip(titlebatch, linkbatch, datebatch, topicbatch, estbatch, labelbatch):\n",
        "              writer.writerow([title, link, date, topic, est, label])\n",
        "    print(f'\\tFinished file {paper}.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file _sz_articles_2019.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'batch_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-05aa1bb01683>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mestbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ]
    }
  ]
}
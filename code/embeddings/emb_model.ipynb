{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "emb_model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
        "outputId": "32a27df5-92f2-436e-c7b2-6d957b9ee2fa"
      },
      "source": [
        "## estimate word embeddings from newspaper data\n",
        "## code adapted from https://github.com/damian0604/embeddingworkshop/blob/main/04exercise.ipynb\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# tqdm allows you to display progress bars in loops\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import gensim\n",
        "\n",
        "# lets get more output\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
        "outputId": "031bbf3e-fe46-4569-d636-0712987322ef"
      },
      "source": [
        "# get full set of news articles\n",
        "!rm sample_data -r\n",
        "!mkdir newspapers\n",
        "!wget -O newspapers/articles.zip https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0\n",
        "!unzip newspapers/articles.zip -d newspapers\n",
        "!rm newspapers/articles.zip"
      ],
      "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "mkdir: cannot create directory ‘newspapers’: File exists\n",
            "--2021-07-21 11:17:19--  https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa [following]\n",
            "--2021-07-21 11:17:19--  https://www.dropbox.com/sh/raw/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca9a52c38306bc1845c7f1e2b3a.dl.dropboxusercontent.com/zip_download_get/A2EPQsvytsO3ASgEJkkrQvLD6QIxOhemBYiePmfMEt2CJLAEL8AEy0_zqKlDSvW17dOjtX1jCx97gedws4uEwYTksuyXxwls1-jSy0AWCBJHuw# [following]\n",
            "--2021-07-21 11:17:20--  https://uca9a52c38306bc1845c7f1e2b3a.dl.dropboxusercontent.com/zip_download_get/A2EPQsvytsO3ASgEJkkrQvLD6QIxOhemBYiePmfMEt2CJLAEL8AEy0_zqKlDSvW17dOjtX1jCx97gedws4uEwYTksuyXxwls1-jSy0AWCBJHuw\n",
            "Resolving uca9a52c38306bc1845c7f1e2b3a.dl.dropboxusercontent.com (uca9a52c38306bc1845c7f1e2b3a.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uca9a52c38306bc1845c7f1e2b3a.dl.dropboxusercontent.com (uca9a52c38306bc1845c7f1e2b3a.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6202295648 (5.8G) [application/zip]\n",
            "Saving to: ‘newspapers/articles.zip’\n",
            "\n",
            "newspapers/articles 100%[===================>]   5.78G  27.1MB/s    in 2m 5s   \n",
            "\n",
            "2021-07-21 11:19:25 (47.4 MB/s) - ‘newspapers/articles.zip’ saved [6202295648/6202295648]\n",
            "\n",
            "Archive:  newspapers/articles.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            "replace newspapers/_sz_articles.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            " extracting: newspapers/_sz_articles.csv  \n",
            " extracting: newspapers/_taz_articles.csv  \n",
            " extracting: newspapers/_faz_articles.csv  \n",
            " extracting: newspapers/_spon_articles.csv  \n",
            " extracting: newspapers/_bild_articles.csv  \n",
            " extracting: newspapers/_sz_articles_2019.csv  \n",
            " extracting: newspapers/_taz_articles_2019.csv  \n",
            " extracting: newspapers/_faz_articles_2019.csv  \n",
            " extracting: newspapers/_bild_articles_2019.csv  \n",
            " extracting: newspapers/_spon_articles_2019.csv  \n",
            " extracting: newspapers/_weltonline_articles.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz3OYCGmcSXU",
        "outputId": "cb60e438-909f-4743-edd4-7cd655ffe26a"
      },
      "source": [
        "# load all texts\n",
        "if 'artcls' in locals():\n",
        "  del(artcls)\n",
        "\n",
        "for filename in tqdm(os.listdir('newspapers')):\n",
        "  if 'artcls' in locals():\n",
        "    print(f'Loaded {artcls.shape[0]} articles', end = '\\r')\n",
        "    artcls = artcls.append(pd.read_csv('newspapers/'+filename))\n",
        "  else:\n",
        "    artcls = pd.read_csv('newspapers/'+filename)\n",
        "print(f'Loaded {artcls.shape[0]} articles, done.')"
      ],
      "id": "lz3OYCGmcSXU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
            "  9%|▉         | 1/11 [00:10<01:43, 10.31s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 263266 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 18%|█▊        | 2/11 [00:57<03:11, 21.23s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 575291 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 27%|██▋       | 3/11 [01:22<03:01, 22.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 794334 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 36%|███▋      | 4/11 [01:28<02:03, 17.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 893180 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 45%|████▌     | 5/11 [02:00<02:10, 21.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 1220020 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 55%|█████▍    | 6/11 [02:06<01:26, 17.23s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 1285387 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 64%|██████▎   | 7/11 [02:20<01:04, 16.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 1436035 articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPBsxGmEgp1W"
      },
      "source": [
        "artcls.text[0]"
      ],
      "id": "wPBsxGmEgp1W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myJ1k4uEvZSb"
      },
      "source": [
        "artcls.shape"
      ],
      "id": "myJ1k4uEvZSb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4w1ixMvg7Zl",
        "outputId": "2347d916-eccd-4eca-af03-9e59eb184333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "\n",
        "# check if string\n",
        "stringvar = [str == type(i) for i in artcls.text]\n",
        "artcls = artcls[stringvar]\n",
        "\n",
        "# cut into sentences\n",
        "trans = str.maketrans('', '', string.punctuation) # translation scheme for removing punctuation\n",
        "uniquesentences = set()\n",
        "for review in tqdm(artcls.text):\n",
        "    for sentence in sent_tokenize(review):\n",
        "        # remove HTML tags in there\n",
        "        sentence = re.sub(r\"<.*?>\",\" \",sentence)\n",
        "        sentence = sentence.translate(trans) \n",
        "        if sentence not in uniquesentences:\n",
        "            uniquesentences.add(sentence.lower())\n",
        "\n",
        "print(f\"We now have {len(uniquesentences)} unique sentences.\")"
      ],
      "id": "p4w1ixMvg7Zl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 239699/257797 [04:31<00:24, 738.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2b0861d69242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# remove HTML tags in there\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"<.*?>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniquesentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0muniquesentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k91FQsjGsLxd"
      },
      "source": [
        "# we do not need a list of lists of tokens later on, so let's use a generator instead of a list to save memory\n",
        "# note that we use round parentheses instead of square brackets to achieve this\n",
        "# we do need two generators, though, as we first need to build the vocabulary and later need to train.\n",
        "# If we use a list, we obviously only need once.\n",
        "tokenizedsentences = (sentence.split() for sentence in uniquesentences)\n",
        "tokenizedsentences2 = (sentence.split() for sentence in uniquesentences)"
      ],
      "id": "k91FQsjGsLxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spkoV479rBV9",
        "outputId": "9a0d8cb7-917e-4c09-b3c6-f5225e9ddebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Started setting up the model at {datetime.now()}\")\n",
        "model = gensim.models.Word2Vec(vector_size=300) # we want 300 dimensions\n",
        "model.build_vocab(tokenizedsentences)\n",
        "print(f\"Started training at {datetime.now()}\")\n",
        "model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=1)\n",
        "# our model gets better if we use more epochs, but we can only do so if we use a list instead of a generator as input\n",
        "# after all, you can only pass over a generator once.\n",
        "# model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=model.epochs)\n",
        "print(f\"Finished training at {datetime.now()}\")"
      ],
      "id": "spkoV479rBV9",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257797, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2XB8dQu6Cl"
      },
      "source": [
        "model.save(\"np_emb\")"
      ],
      "id": "J-2XB8dQu6Cl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3OV2lKvDdT"
      },
      "source": [
        ""
      ],
      "id": "hC3OV2lKvDdT",
      "execution_count": null,
      "outputs": []
    }
  ]
}
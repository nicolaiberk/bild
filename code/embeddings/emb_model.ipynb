{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "emb_model.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
        "outputId": "ace67542-3a3c-4659-fb47-62eb82f4d487"
      },
      "source": [
        "## estimate word embeddings from newspaper data\n",
        "## code adapted from https://github.com/damian0604/embeddingworkshop/blob/main/04exercise.ipynb\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# tqdm allows you to display progress bars in loops\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import gensim\n",
        "\n",
        "# lets get more output\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "id": "c5825374-c511-47fe-a2e4-da9bd8431a6d",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
        "outputId": "cbf5d296-128f-403a-d7a0-a88cfe244add"
      },
      "source": [
        "# get full set of news articles\n",
        "!rm sample_data -r\n",
        "!mkdir newspapers\n",
        "!wget -O newspapers/articles.zip https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0\n",
        "!unzip newspapers/articles.zip -d newspapers\n",
        "!rm newspapers/articles.zip"
      ],
      "id": "11c85d9f-4750-470f-8ff8-a0a91dc705c6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-21 11:39:46--  https://www.dropbox.com/sh/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa [following]\n",
            "--2021-07-21 11:39:46--  https://www.dropbox.com/sh/raw/r6k4qk9flgz0agu/AAA5ZLsuOwk9UWiEsLAOFmDSa\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc61d53532e3c4c0e875eaf0c351.dl.dropboxusercontent.com/zip_download_get/A2Fc0D8YvMcR4I0Gt-Gr2_Cxe15W_-nJpwg4zXuhx2TLW6bGPERJWupAuT58O1JZVZ-QNo-EjEgGR0TlopF4bf-ZJ6g-drSFk-T6weF4gnpZzw# [following]\n",
            "--2021-07-21 11:39:47--  https://uc61d53532e3c4c0e875eaf0c351.dl.dropboxusercontent.com/zip_download_get/A2Fc0D8YvMcR4I0Gt-Gr2_Cxe15W_-nJpwg4zXuhx2TLW6bGPERJWupAuT58O1JZVZ-QNo-EjEgGR0TlopF4bf-ZJ6g-drSFk-T6weF4gnpZzw\n",
            "Resolving uc61d53532e3c4c0e875eaf0c351.dl.dropboxusercontent.com (uc61d53532e3c4c0e875eaf0c351.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc61d53532e3c4c0e875eaf0c351.dl.dropboxusercontent.com (uc61d53532e3c4c0e875eaf0c351.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6202295648 (5.8G) [application/zip]\n",
            "Saving to: ‘newspapers/articles.zip’\n",
            "\n",
            "newspapers/articles 100%[===================>]   5.78G  13.1MB/s    in 6m 5s   \n",
            "\n",
            "2021-07-21 11:45:53 (16.2 MB/s) - ‘newspapers/articles.zip’ saved [6202295648/6202295648]\n",
            "\n",
            "Archive:  newspapers/articles.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: newspapers/_sz_articles.csv  \n",
            " extracting: newspapers/_taz_articles.csv  \n",
            " extracting: newspapers/_faz_articles.csv  \n",
            " extracting: newspapers/_spon_articles.csv  \n",
            " extracting: newspapers/_bild_articles.csv  \n",
            " extracting: newspapers/_sz_articles_2019.csv  \n",
            " extracting: newspapers/_taz_articles_2019.csv  \n",
            " extracting: newspapers/_faz_articles_2019.csv  \n",
            " extracting: newspapers/_bild_articles_2019.csv  \n",
            " extracting: newspapers/_spon_articles_2019.csv  \n",
            " extracting: newspapers/_weltonline_articles.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz3OYCGmcSXU",
        "outputId": "0cef9cf1-be17-4d08-8229-a6c3a7f545a1"
      },
      "source": [
        "# load all texts\n",
        "if 'artcls' in locals():\n",
        "  del(artcls)\n",
        "\n",
        "for filename in tqdm(os.listdir('newspapers')):\n",
        "  if 'artcls' in locals():\n",
        "    print(f'\\nLoaded {artcls.shape[0]} articles')\n",
        "    artcls = artcls.append(pd.read_csv('newspapers/'+filename))\n",
        "  else:\n",
        "    artcls = pd.read_csv('newspapers/'+filename)\n",
        "print(f'Loaded {artcls.shape[0]} articles, done.')"
      ],
      "id": "lz3OYCGmcSXU",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1/11 [00:06<01:09,  6.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 263266 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 2/11 [00:19<01:17,  8.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 575291 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 3/11 [00:26<01:05,  8.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 794334 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 4/11 [00:28<00:44,  6.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 893180 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 5/11 [00:37<00:42,  7.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1220020 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 55%|█████▍    | 6/11 [00:39<00:28,  5.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1285387 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▎   | 7/11 [00:47<00:25,  6.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1436035 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 8/11 [00:49<00:14,  4.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1472453 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 9/11 [00:51<00:08,  4.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1545861 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 10/11 [00:53<00:03,  3.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded 1634513 articles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [01:08<00:00,  6.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 2474182 articles, done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "wPBsxGmEgp1W",
        "outputId": "e56099a5-f280-4cb8-d68e-615bd7b81111"
      },
      "source": [
        "artcls = artcls.reset_index()\n",
        "artcls.text[0]"
      ],
      "id": "wPBsxGmEgp1W",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was fehlt ... ... der Vorsatz    Der Vorsatz hat keinen guten Klang: Mit Vorsatz gehandelt zu haben, wird einem meist im Gericht vorgeworfen. Auch die guten Vorsätze haben immer den unangenehmen Beigeschmack von Schuld und Sühne - nicht zuletzt, weil sie zu 90 Prozent gebrochen werden. Woher der Brauch kommt, sich im neuen Jahr eine Änderung des Verhaltens vorzunehmen, ist unklar.    Am wahrscheinlichsten ist ein christlicher Ursprung, wie bei vielen Festtagsbräuchen - immerhin stammt das Wort Silvester vom Namenstag des Papstes Silvester (lateinisch für \"Waldmensch\"), der am 31. Dezember 335 starb. Möglicherweise sind die guten Vorsätze also eine katholische Erfindung: Die Sünden werden vergeben, aber nur, wenn man Besserung gelobt.    Die Wortherkunft der guten Vorsätze ist leichter zu bestimmen: Die Wurzel des Guten liegt im germanischen \"goda\" (passend, geeignet), das sich im 8. Jahrhundert zu \"guot\" (Besitz, Vermögen) weiterentwickelte. Vorsätze hießen im Mittelhochdeutschen \"vürsaz\" (Vorhaben, Absicht) und sind vom althochdeutschen \"sezzen\" (aufstellen, festlegen) abgeleitet, welches wiederum vom germanischen \"set-ja\" (sitzen) abstammt.    Was man gut findet, also was einem gerade \"passt\", kann morgen schon wieder stören, wie es mit den guten Vorsätzen meist ist. Schöner als das säuerlich-christliche Bekenntnis zur Besserung wäre es ohnehin, die Tabula rasa des neuen Jahres zu nutzen, um den in einigen Kulturen verbreiteten Brauch zu praktizieren, allen Streit und Ärger des vergangenen Jahres zu vergessen, alle Schuld zu erlassen, alle Fehler zu vergeben - ohne Gegenleistung. Wäre das nicht mal ein guter Vorsatz? (WENK)    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myJ1k4uEvZSb",
        "outputId": "ce58021a-74a1-49b4-c98c-b85cb3a3d3c3"
      },
      "source": [
        "artcls.shape"
      ],
      "id": "myJ1k4uEvZSb",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2474182, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4w1ixMvg7Zl",
        "outputId": "39863cd4-f44a-4934-cb72-d289e33dfea6"
      },
      "source": [
        "# check if string\n",
        "stringvar = [str == type(i) for i in artcls.text]\n",
        "artcls = artcls[stringvar]\n",
        "\n",
        "# cut into sentences\n",
        "trans = str.maketrans('', '', string.punctuation) # translation scheme for removing punctuation\n",
        "uniquesentences = set()\n",
        "for review in tqdm(artcls.text):\n",
        "    for sentence in sent_tokenize(review):\n",
        "        # remove HTML tags in there\n",
        "        sentence = re.sub(r\"<.*?>\",\" \",sentence)\n",
        "        sentence = sentence.translate(trans) \n",
        "        if sentence not in uniquesentences:\n",
        "            uniquesentences.add(sentence.lower())\n",
        "\n",
        "print(f\"We now have {len(uniquesentences)} unique sentences.\")"
      ],
      "id": "p4w1ixMvg7Zl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 341824/2214853 [07:07<48:40, 641.26it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrSZs-UZ5UoU"
      },
      "source": [
        "del(artcls)"
      ],
      "id": "OrSZs-UZ5UoU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k91FQsjGsLxd"
      },
      "source": [
        "# we do not need a list of lists of tokens later on, so let's use a generator instead of a list to save memory\n",
        "# note that we use round parentheses instead of square brackets to achieve this\n",
        "# we do need two generators, though, as we first need to build the vocabulary and later need to train.\n",
        "# If we use a list, we obviously only need once.\n",
        "tokenizedsentences = (sentence.split() for sentence in uniquesentences)\n",
        "tokenizedsentences2 = (sentence.split() for sentence in uniquesentences)"
      ],
      "id": "k91FQsjGsLxd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spkoV479rBV9",
        "outputId": "9a0d8cb7-917e-4c09-b3c6-f5225e9ddebf"
      },
      "source": [
        "print(f\"Started setting up the model at {datetime.now()}\")\n",
        "model = gensim.models.Word2Vec(vector_size=300) # we want 300 dimensions\n",
        "model.build_vocab(tokenizedsentences)\n",
        "print(f\"Started training at {datetime.now()}\")\n",
        "model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=1)\n",
        "# our model gets better if we use more epochs, but we can only do so if we use a list instead of a generator as input\n",
        "# after all, you can only pass over a generator once.\n",
        "# model.train(tokenizedsentences2, total_examples=model.corpus_count,  epochs=model.epochs)\n",
        "print(f\"Finished training at {datetime.now()}\")"
      ],
      "id": "spkoV479rBV9",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257797, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2XB8dQu6Cl"
      },
      "source": [
        "model.save(\"np_emb\")"
      ],
      "id": "J-2XB8dQu6Cl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3OV2lKvDdT"
      },
      "source": [
        ""
      ],
      "id": "hC3OV2lKvDdT",
      "execution_count": null,
      "outputs": []
    }
  ]
}
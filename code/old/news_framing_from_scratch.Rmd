---
title: "Migration Framing"
author: "Nicolai Berk"
date: "7 8 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)

library(dplyr)
library(tidyverse)
library(data.table)
library(here)
library(stm)
library(lubridate)
library(tidytext)
library(stopwords)

```

Since factor analysis, correspondence analysis and the structural topic model failed to run on the bi- & trigram matrix, I will estimate a classic structural topic model (yes, I can hear you booing) in the hope to get a little fancier with word embeddings later. I follow Julia Silge's tutorial [here](https://juliasilge.com/blog/sherlock-holmes-stm/).

```{r load}

# load the preselected migration articles
mig_texts_raw <- fread(here('data/_migration_articles_new.csv'), encoding = 'UTF-8') %>% as_tibble()

# use subsample for now
# mig_texts_raw <- mig_texts_raw[sample(1:nrow(mig_texts_raw), 10000),]

# fix dates
mig_texts_raw$paper[mig_texts_raw$paper == "weltonline"] <- "welt"
mig_texts_raw$date_new <- NA_Date_
mig_texts_raw$date_new[mig_texts_raw$paper == "bild"] <- mig_texts_raw$date[mig_texts_raw$paper == "bild"] %>% as.Date()
mig_texts_raw$date_new[mig_texts_raw$paper == "faz"] <- mig_texts_raw$date[mig_texts_raw$paper=="faz"] %>% as.numeric() %>% as.Date(origin = "1970-01-01")
mig_texts_raw$date_new[mig_texts_raw$paper == "spon"] <- mig_texts_raw$date[mig_texts_raw$paper=="spon"] %>% as.Date(format = "%d.%m.%Y")
mig_texts_raw$date_new[mig_texts_raw$paper == "sz"] <- mig_texts_raw$date[mig_texts_raw$paper=="sz"] %>% as.integer() %>% as.Date(origin = "1970-01-01")
mig_texts_raw$date_new[mig_texts_raw$paper == "taz"] <- mig_texts_raw$date[mig_texts_raw$paper=="taz"] %>% as.integer() %>% as.Date(origin = "1970-01-01")
mig_texts_raw$date_new[mig_texts_raw$paper == "welt"] <- mig_texts_raw$date[mig_texts_raw$paper=="welt"] %>% as.integer() %>% as.Date(origin = "1970-01-01")
mig_texts_raw <- mig_texts_raw[mig_texts_raw$date_new > as.Date('2010-01-01'),]

## check uniqueness of articles
# length(unique(mig_texts_raw$url)) # some duplicates

## load german stopwords
german_stopwords <- data.frame(word = stopwords("de"), stringsAsFactors = F)

## preprocess: sort out non-unique cases, tokenize, remove stopwords
mig_texts_tidy <- mig_texts_raw %>%
  group_by(url) %>% 
  summarise(date_new = first(date_new),
            paper = first(paper),
            title = first(title),
            text = first(text)
            ) %>%
  mutate(article = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(german_stopwords) %>% 
  filter(!word %in% c('dass', 'sagte', 'sagt', 'sei')) # missing in stopwords

mig_dfm <- mig_texts_tidy %>% 
  count(article, word, sort = T) %>% 
  cast_dfm(article, word, n)


topic_model <- stm(mig_dfm, K = 100, init.type = 'Spectral') # k = 0 chooses optimal number of topics 
summary(topic_model)

```

